{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b624ead-ea87-4a8b-99e2-d3432b7ba6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3549.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 400/400 [00:00<00:00, 52280.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 100/100 [00:00<00:00, 21565.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集已保存到 ../user_data/step1/kk/dataset 中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################################\n",
    "###                                             Step2: kk                                                         ###\n",
    "#####################################################################################################################\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import DatasetDict, Dataset\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../user_data/bart_tokenizer\")\n",
    "\n",
    "# 构建双语数据集\n",
    "# 读取文件，构造数据列表\n",
    "source_data = []\n",
    "with open(\"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/val/中文/kk-zh.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        source_text = line.strip()\n",
    "        # 添加特殊token\n",
    "        source_text = f\"<zh> {source_text} </s>\"\n",
    "        source_data.append(source_text)\n",
    "target_data = []\n",
    "with open(\"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/val/其他语言/kk-zh.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        target_text = line.strip()\n",
    "        # 添加特殊token\n",
    "        target_text = f\"<kk> {target_text} </s>\"\n",
    "        target_data.append(target_text)\n",
    "\n",
    "# Tokenize 函数\n",
    "def tokenize_function(examples):\n",
    "    source_texts = examples[\"source\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "\n",
    "    # Tokenize source texts\n",
    "    model_inputs = tokenizer(source_texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    \n",
    "    # Tokenize target texts without using as_target_tokenizer context\n",
    "    labels = tokenizer(target_texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    # 将 labels 直接添加到 model_inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # 转换成字典格式，便于 datasets 库使用\n",
    "    return {key: value.tolist() for key, value in model_inputs.items()}\n",
    "\n",
    "\n",
    "data = []\n",
    "for source_text, target_text in zip(source_data, target_data):\n",
    "    data.append({\"source\": source_text, \"target\": target_text})\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "# 划分数据集\n",
    "tokenized_train_dataset, tokenized_val_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42).values()\n",
    "# 定义保存路径\n",
    "output_dir = \"../user_data/step1/kk/dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 保存 tokenized 数据集\n",
    "tokenized_train_dataset.save_to_disk(os.path.join(output_dir, \"train\"))\n",
    "tokenized_val_dataset.save_to_disk(os.path.join(output_dir, \"val\"))\n",
    "\n",
    "print(\"数据集已保存到 ../user_data/step1/kk/dataset 中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9e5f03-0efc-4acb-ba09-61a815f3f0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-09 17:11:09.053332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 17:11:09.075870: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 17:11:09.082865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 17:11:09.102673: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 17:11:10.030999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer is done!\n",
      "model is done!\n",
      "data is done!\n",
      "训练参数已设置完成！\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [70/70 00:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.158400</td>\n",
       "      <td>2.256412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.204100</td>\n",
       "      <td>1.933548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.830100</td>\n",
       "      <td>1.738075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.667400</td>\n",
       "      <td>1.642027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.481500</td>\n",
       "      <td>1.591474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.383700</td>\n",
       "      <td>1.563963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.266600</td>\n",
       "      <td>1.544860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.253800</td>\n",
       "      <td>1.539731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.163100</td>\n",
       "      <td>1.533865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.172400</td>\n",
       "      <td>1.532473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=70, training_loss=1.6581157548086984, metrics={'train_runtime': 37.3639, 'train_samples_per_second': 107.055, 'train_steps_per_second': 1.873, 'total_flos': 212984659968000.0, 'train_loss': 1.6581157548086984, 'epoch': 10.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. 加载tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../user_data/bart_tokenizer\")\n",
    "print(\"tokenizer is done!\")\n",
    "\n",
    "# 2. 加载模型\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/ru/results/checkpoint-30940\")\n",
    "print(\"model is done!\")\n",
    "\n",
    "\n",
    "# 5. 加载数据\n",
    "dataset_dir = \"../user_data/step1/kk/dataset\"\n",
    "tokenized_train_dataset = load_from_disk(f\"{dataset_dir}/train\")\n",
    "tokenized_val_dataset = load_from_disk(f\"{dataset_dir}/val\")\n",
    "tokenized_train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "print(\"data is done!\")\n",
    "\n",
    "# 6. 设置训练参数\n",
    "output_dir = \"../user_data/step1/kk\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"results\"),         # 训练结果保存路径\n",
    "    eval_strategy=\"epoch\",                            # 按步数进行评估\n",
    "    save_strategy=\"epoch\",                                   # 按步数进行保存\n",
    "    logging_dir=os.path.join(output_dir, \"logs\"),           # 日志保存路径\n",
    "    logging_strategy=\"epoch\",                                     # 日志打印间隔\n",
    "    learning_rate=1e-4,                                     # 学习率\n",
    "    per_device_train_batch_size=64,                         # 每个设备的训练批次大小\n",
    "    per_device_eval_batch_size=256,                         # 每个设备的验证批次大小\n",
    "    weight_decay=0.01,                                      # 权重衰减\n",
    "    save_total_limit=3,                                    # 保存的 checkpoint 数量上限\n",
    "    num_train_epochs=10,                                     # 训练 epoch 数\n",
    "    predict_with_generate=True,                             # 使用生成模式进行评估\n",
    "    bf16=True,                                              # 使用 bf16 精度\n",
    "    load_best_model_at_end=True,                            # 训练结束后加载最好的模型\n",
    "    metric_for_best_model=\"eval_loss\",                      # 最好模型的评估指标\n",
    "    greater_is_better=False,                                # 对于 Loss，越小越好\n",
    ")\n",
    "\n",
    "print(\"训练参数已设置完成！\")\n",
    "\n",
    "\n",
    "# 7. 使用 Seq2SeqTrainer 进行 微调\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                                     # 模型\n",
    "    args=training_args,                              # 训练参数\n",
    "    train_dataset=tokenized_train_dataset,           # 训练数据集\n",
    "    eval_dataset=tokenized_val_dataset,              # 验证数据集\n",
    "    tokenizer=tokenizer,                             # 分词器\n",
    ")\n",
    "\n",
    "# 8. 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81ae5a-5f16-44ca-8863-f27244f7fbd2",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7f534f-af41-48c4-be64-b3b5ce64ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "import sacrebleu\n",
    "from datasets import Dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "# 设置参数\n",
    "batch_size = 32\n",
    "beam_size = 8\n",
    "\n",
    "\n",
    "# 5. 加载数据\n",
    "dataset_dir = \"../user_data/step1/kk/dataset\"\n",
    "tokenized_train_dataset = load_from_disk(f\"{dataset_dir}/train\")\n",
    "tokenized_val_dataset = load_from_disk(f\"{dataset_dir}/val\")\n",
    "print(\"data is done!\")\n",
    "model_output_dir = \"../user_data/bart_tokenizer\"\n",
    "# 加载tokenizer和model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_output_dir)\n",
    "print(\"Tokenizer loaded.\")\n",
    "model = trainer.model\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4d16dd8-d027-436d-8260-d712bc8d7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 score: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义翻译函数\n",
    "def translate_batch(batch):\n",
    "    inputs = tokenizer(batch['source'], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    # 去掉token_type_ids，只保留input_ids和attention_mask\n",
    "    inputs = {key: inputs[key] for key in ['input_ids', 'attention_mask']}\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        num_beams=beam_size,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=model.config.bos_token_id,  # 使用起始标记\n",
    "    )\n",
    "    translated_texts = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "# 创建保存预测结果和真实结果的列表\n",
    "predictions = []\n",
    "references = tokenized_val_dataset['target']\n",
    "dataset = DataLoader(tokenized_val_dataset, batch_size=50)\n",
    "# 分批处理并翻译\n",
    "for batch in tqdm(dataset):\n",
    "    batch_predictions = translate_batch(batch)\n",
    "    predictions.extend([i.strip() for i in batch_predictions])\n",
    "\n",
    "# 计算BLEU分数\n",
    "bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "print(f\"BLEU-4 score: {bleu.score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537f050-307d-403b-882f-824ef4cab8b8",
   "metadata": {},
   "source": [
    "# 全量数据微调15个epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac6f3cf-94e9-4bb0-8e44-32eab548ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer is done!\n",
      "model is done!\n",
      "data is done!\n",
      "训练参数已设置完成！\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:55, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.152900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.702200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.491300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.983600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.886100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.851500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.822600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.802100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=1.2806284546852111, metrics={'train_runtime': 55.6214, 'train_samples_per_second': 134.84, 'train_steps_per_second': 2.157, 'total_flos': 399346237440000.0, 'train_loss': 1.2806284546852111, 'epoch': 15.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. 加载tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../user_data/bart_tokenizer\")\n",
    "print(\"tokenizer is done!\")\n",
    "\n",
    "# 2. 加载模型\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/ru/results/checkpoint-30940\")\n",
    "print(\"model is done!\")\n",
    "\n",
    "\n",
    "# 5. 加载数据\n",
    "dataset_dir = \"../user_data/step1/kk/dataset\"\n",
    "tokenized_train_dataset = load_from_disk(f\"{dataset_dir}/train\")\n",
    "tokenized_val_dataset = load_from_disk(f\"{dataset_dir}/val\")\n",
    "tokenized_train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "# 拼接两个数据集\n",
    "tokenized_dataset = concatenate_datasets([tokenized_train_dataset, tokenized_val_dataset])\n",
    "\n",
    "print(\"data is done!\")\n",
    "\n",
    "# 6. 设置训练参数\n",
    "output_dir = \"../user_data/step1/kk\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"results\"),         # 训练结果保存路径\n",
    "    save_strategy=\"epoch\",                                   # 按步数进行保存\n",
    "    logging_dir=os.path.join(output_dir, \"logs\"),           # 日志保存路径\n",
    "    logging_strategy=\"epoch\",                                     # 日志打印间隔\n",
    "    learning_rate=1e-4,                                     # 学习率\n",
    "    per_device_train_batch_size=64,                         # 每个设备的训练批次大小\n",
    "    weight_decay=0.01,                                      # 权重衰减\n",
    "    save_total_limit=3,                                    # 保存的 checkpoint 数量上限\n",
    "    num_train_epochs=15,                                     # 训练 epoch 数\n",
    "    bf16=True,                                              # 使用 bf16 精度\n",
    ")\n",
    "\n",
    "print(\"训练参数已设置完成！\")\n",
    "\n",
    "\n",
    "# 7. 使用 Seq2SeqTrainer 进行 微调\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                                     # 模型\n",
    "    args=training_args,                              # 训练参数\n",
    "    train_dataset=tokenized_dataset,           # 训练数据集\n",
    "    tokenizer=tokenizer,                             # 分词器\n",
    ")\n",
    "\n",
    "# 8. 开始训练\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
