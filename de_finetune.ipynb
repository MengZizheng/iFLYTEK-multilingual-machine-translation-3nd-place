{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9e5f03-0efc-4acb-ba09-61a815f3f0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 15:57:51.181963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 15:57:51.200788: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 15:57:51.206743: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 15:57:51.223809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 15:57:52.010519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer is done!\n",
      "model is done!\n",
      "data is done!\n",
      "训练参数已设置完成！\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15470' max='15470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15470/15470 29:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.439500</td>\n",
       "      <td>1.086582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.072000</td>\n",
       "      <td>0.935476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.958800</td>\n",
       "      <td>0.858483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.810336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.851600</td>\n",
       "      <td>0.776070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.822800</td>\n",
       "      <td>0.749787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.758100</td>\n",
       "      <td>0.731573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.716576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.725900</td>\n",
       "      <td>0.703194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.714300</td>\n",
       "      <td>0.689517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.704200</td>\n",
       "      <td>0.677929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.697300</td>\n",
       "      <td>0.669388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.657500</td>\n",
       "      <td>0.665439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.634400</td>\n",
       "      <td>0.658592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.631400</td>\n",
       "      <td>0.652338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.644580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.630600</td>\n",
       "      <td>0.640661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.623400</td>\n",
       "      <td>0.634655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.602200</td>\n",
       "      <td>0.632980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.578400</td>\n",
       "      <td>0.630475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>0.627014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.623823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>0.619823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>0.617117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.564100</td>\n",
       "      <td>0.617024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.540800</td>\n",
       "      <td>0.616053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.540900</td>\n",
       "      <td>0.615134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.540800</td>\n",
       "      <td>0.612870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.611622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.610522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15470, training_loss=0.6935355543396746, metrics={'train_runtime': 1749.039, 'train_samples_per_second': 566.025, 'train_steps_per_second': 8.845, 'total_flos': 5.271370334208e+16, 'train_loss': 0.6935355543396746, 'epoch': 5.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. 加载tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../user_data/bart_tokenizer\")\n",
    "print(\"tokenizer is done!\")\n",
    "\n",
    "# 2. 加载模型(用en的模型)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/en/results/checkpoint-154690\")\n",
    "print(\"model is done!\")\n",
    "\n",
    "\n",
    "# 5. 加载数据\n",
    "dataset_dir = \"../user_data/step1/de/dataset\"\n",
    "tokenized_train_dataset = load_from_disk(f\"{dataset_dir}/train\")\n",
    "tokenized_val_dataset = load_from_disk(f\"{dataset_dir}/val\")\n",
    "tokenized_train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "print(\"data is done!\")\n",
    "\n",
    "# 6. 设置训练参数\n",
    "output_dir = \"../user_data/step1/de\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"results\"),         # 训练结果保存路径\n",
    "    eval_strategy=\"steps\",                            # 按步数进行评估\n",
    "    save_strategy=\"steps\",                                   # 按步数进行保存\n",
    "    logging_dir=os.path.join(output_dir, \"logs\"),           # 日志保存路径\n",
    "    logging_steps=500,                                     # 日志打印间隔\n",
    "    eval_steps=500,                                        # 每 500 步进行一次评估\n",
    "    save_steps=500,\n",
    "    learning_rate=1e-4,                                     # 学习率\n",
    "    per_device_train_batch_size=64,                         # 每个设备的训练批次大小\n",
    "    per_device_eval_batch_size=256,                         # 每个设备的验证批次大小\n",
    "    weight_decay=0.01,                                      # 权重衰减\n",
    "    save_total_limit=3,                                    # 保存的 checkpoint 数量上限\n",
    "    num_train_epochs=5,                                     # 训练 epoch 数\n",
    "    predict_with_generate=True,                             # 使用生成模式进行评估\n",
    "    bf16=True,                                              # 使用 bf16 精度\n",
    "    load_best_model_at_end=True,                            # 训练结束后加载最好的模型\n",
    "    metric_for_best_model=\"eval_loss\",                      # 最好模型的评估指标\n",
    "    greater_is_better=False,                                # 对于 Loss，越小越好\n",
    ")\n",
    "\n",
    "print(\"训练参数已设置完成！\")\n",
    "\n",
    "\n",
    "# 7. 使用 Seq2SeqTrainer 进行 微调\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                                     # 模型\n",
    "    args=training_args,                              # 训练参数\n",
    "    train_dataset=tokenized_train_dataset,           # 训练数据集\n",
    "    eval_dataset=tokenized_val_dataset,              # 验证数据集\n",
    "    tokenizer=tokenizer,                             # 分词器\n",
    ")\n",
    "\n",
    "# 8. 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81ae5a-5f16-44ca-8863-f27244f7fbd2",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7f534f-af41-48c4-be64-b3b5ce64ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "Tokenizer loaded.\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "import sacrebleu\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "# 设置参数\n",
    "batch_size = 32\n",
    "beam_size = 8\n",
    "\n",
    "\n",
    "# 读取source和target文件\n",
    "def load_txt_data(source_path, target_path):\n",
    "    with open(source_path, \"r\", encoding=\"utf-8\") as src_file, open(target_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        source_sentences = [f\"<zh> {line.strip()} </s>\" for line in src_file.readlines()]\n",
    "        target_sentences = [line.strip() for line in tgt_file.readlines()]\n",
    "    return source_sentences, target_sentences\n",
    "\n",
    "data_files = {\n",
    "    \"source\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/val/中文/de-zh.txt\",\n",
    "    \"target\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/val/其他语言/de-zh.txt\"\n",
    "}\n",
    "\n",
    "# 加载txt文件中的句子\n",
    "source_sentences, target_sentences = load_txt_data(data_files[\"source\"], data_files[\"target\"])\n",
    "\n",
    "# 将数据转换为datasets格式\n",
    "dataset_dict = {\"source\": source_sentences, \"target\": target_sentences}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"Data loaded.\")\n",
    "model_output_dir = \"../user_data/step1/de/results/checkpoint-15470\"\n",
    "# 加载tokenizer和model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_output_dir)\n",
    "print(\"Tokenizer loaded.\")\n",
    "model = BartForConditionalGeneration.from_pretrained(model_output_dir).eval().to(device)\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d16dd8-d027-436d-8260-d712bc8d7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:46<00:00,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 score: 9.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义翻译函数\n",
    "def translate_batch(batch):\n",
    "    inputs = tokenizer(batch['source'], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    # 去掉token_type_ids，只保留input_ids和attention_mask\n",
    "    inputs = {key: inputs[key] for key in ['input_ids', 'attention_mask']}\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        num_beams=beam_size,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=model.config.bos_token_id,  # 使用起始标记\n",
    "    )\n",
    "    translated_texts = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "# 创建保存预测结果和真实结果的列表\n",
    "predictions = []\n",
    "references = dataset['target']\n",
    "\n",
    "# 分批处理并翻译\n",
    "for batch in tqdm(dataloader):\n",
    "    batch_predictions = translate_batch(batch)\n",
    "    predictions.extend([i.strip() for i in batch_predictions])\n",
    "\n",
    "# 计算BLEU分数\n",
    "bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "print(f\"BLEU-4 score: {bleu.score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
