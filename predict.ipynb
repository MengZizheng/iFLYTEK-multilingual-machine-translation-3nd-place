{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16a8238-b813-41f8-a384-d1340dea40c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3153.98 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3154.19 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3213.74 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3215.46 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3240.13 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 3077.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "from datasets import DatasetDict, Dataset\n",
    "import os\n",
    "\n",
    "\n",
    "# 数据集字典\n",
    "dataset_dict = DatasetDict()\n",
    "# 定义双语数据集路径\n",
    "data_files = {\n",
    "    \"en-zh\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/test/中文/en-zh.txt\",\n",
    "    \"de-zh\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/test/中文/de-zh.txt\",\n",
    "    \"ru-zh\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/test/中文/ru-zh.txt\",\n",
    "    \"es-zh\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/test/中文/es-zh.txt\",\n",
    "    \"ja-zh\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/test/中文/ja-zh.txt\",\n",
    "    \"kk-zh\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/test/中文/kk-zh.txt\",\n",
    "}\n",
    "# 加载分词器\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../user_data/bart_tokenizer\")\n",
    "\n",
    "for target_lang in [\"en\", \"de\", \"ru\", \"es\", \"ja\", \"kk\"]:\n",
    "    # 读取文件，构造数据列表\n",
    "    data = []\n",
    "    with open(data_files[f\"{target_lang}-zh\"], \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            source_text = line.strip()\n",
    "            # 添加特殊token，确保 source 是中文\n",
    "            source_text = f\"<zh> {source_text} </s>\"\n",
    "            data.append({\"source\": source_text})\n",
    "    # 创建Dataset对象\n",
    "    dataset = Dataset.from_list(data)\n",
    "    dataset_dict[f\"{target_lang}-zh\"] = dataset\n",
    "\n",
    "# Tokenize 函数\n",
    "def tokenize_function(examples):\n",
    "    source_texts = examples[\"source\"]\n",
    "    # Tokenize source texts\n",
    "    model_inputs = tokenizer(source_texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    # 转换成字典格式，便于 datasets 库使用\n",
    "    return {key: value.tolist() for key, value in model_inputs.items()}\n",
    "\n",
    "tokenized_dataset_dict = dataset_dict.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169cbdeb-6f44-410c-9350-2a6799e5f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "batch_size = 32\n",
    "beam_size = 8\n",
    "# 定义翻译函数\n",
    "def translate_batch(batch):\n",
    "    inputs = tokenizer(batch['source'], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    # 去掉token_type_ids，只保留input_ids和attention_mask\n",
    "    inputs = {key: inputs[key] for key in ['input_ids', 'attention_mask']}\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_length=128,\n",
    "        num_beams=beam_size,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=model.config.bos_token_id,  # 使用起始标记\n",
    "    )\n",
    "    translated_texts = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d326cbff-8462-48be-8a45-1162cb6b3fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-zh\n",
      "model is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:18<00:00, 15.62s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"en-zh\")\n",
    "dataloader = DataLoader(dataset_dict[\"en-zh\"], batch_size=100)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/en/results/checkpoint-154690\").eval().to(device)\n",
    "print(\"model is done!\")\n",
    "\n",
    "predictions = []\n",
    "for batch in tqdm(dataloader):\n",
    "    translated_texts = translate_batch(batch)\n",
    "    predictions.extend(translated_texts)\n",
    "with open(\"../prediction_result/submit/en-zh.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in predictions:\n",
    "        f.write(f\"{line}\\n\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "334cbff3-3ee2-4823-ac3e-92e90b8d4ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-zh\n",
      "model is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:42<00:00,  8.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"en-zh\")\n",
    "dataloader = DataLoader(dataset_dict[\"de-zh\"], batch_size=100)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/de/results/checkpoint-15470\").eval().to(device)\n",
    "print(\"model is done!\")\n",
    "\n",
    "predictions = []\n",
    "for batch in tqdm(dataloader):\n",
    "    translated_texts = translate_batch(batch)\n",
    "    predictions.extend(translated_texts)\n",
    "with open(\"../prediction_result/submit/de-zh.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in predictions:\n",
    "        f.write(f\"{line}\\n\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f86299-0b3f-4d49-ab4a-ae36122d2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-zh\n",
      "model is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:40<00:00,  8.18s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"en-zh\")\n",
    "dataloader = DataLoader(dataset_dict[\"es-zh\"], batch_size=100)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/es/results/checkpoint-15470\").eval().to(device)\n",
    "print(\"model is done!\")\n",
    "\n",
    "predictions = []\n",
    "for batch in tqdm(dataloader):\n",
    "    translated_texts = translate_batch(batch)\n",
    "    predictions.extend(translated_texts)\n",
    "with open(\"../prediction_result/submit/es-zh.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in predictions:\n",
    "        f.write(f\"{line}\\n\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eec155e2-13f7-4954-a50a-1f0d293f360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ja-zh\n",
      "model is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:34<00:00,  6.81s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"ja-zh\")\n",
    "dataloader = DataLoader(dataset_dict[\"ja-zh\"], batch_size=100)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/ja/results/checkpoint-120\").eval().to(device)\n",
    "print(\"model is done!\")\n",
    "\n",
    "predictions = []\n",
    "for batch in tqdm(dataloader):\n",
    "    translated_texts = translate_batch(batch)\n",
    "    predictions.extend(translated_texts)\n",
    "with open(\"../prediction_result/submit/ja-zh.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in predictions:\n",
    "        f.write(f\"{line}\\n\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "115a3109-4456-4520-b6ec-73ad69d21234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk-zh\n",
      "model is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.82s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"kk-zh\")\n",
    "dataloader = DataLoader(dataset_dict[\"kk-zh\"], batch_size=100)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/kk/results/checkpoint-120\").eval().to(device)\n",
    "print(\"model is done!\")\n",
    "\n",
    "predictions = []\n",
    "for batch in tqdm(dataloader):\n",
    "    translated_texts = translate_batch(batch)\n",
    "    predictions.extend(translated_texts)\n",
    "with open(\"../prediction_result/submit/kk-zh.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in predictions:\n",
    "        f.write(f\"{line}\\n\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "255167ce-75aa-4335-aecd-c16a1d6e219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru-zh\n",
      "model is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:40<00:00,  8.10s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"ru-zh\")\n",
    "dataloader = DataLoader(dataset_dict[\"ru-zh\"], batch_size=100)\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/ru/results/checkpoint-30940\").eval().to(device)\n",
    "print(\"model is done!\")\n",
    "\n",
    "predictions = []\n",
    "for batch in tqdm(dataloader):\n",
    "    translated_texts = translate_batch(batch)\n",
    "    predictions.extend(translated_texts)\n",
    "with open(\"../prediction_result/submit/ru-zh.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in predictions:\n",
    "        f.write(f\"{line}\\n\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bad1a807-5518-44d4-8096-c1d8e186911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and processing each file by stripping each line and saving the results\n",
    "file_paths = [\n",
    "    \"../prediction_result/submit/en-zh.txt\",\n",
    "    \"../prediction_result/submit/ru-zh.txt\",\n",
    "    \"../prediction_result/submit/ja-zh.txt\",\n",
    "    \"../prediction_result/submit/kk-zh.txt\",\n",
    "    \"../prediction_result/submit/de-zh.txt\",\n",
    "    \"../prediction_result/submit/es-zh.txt\"\n",
    "]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Read the file, strip each line, and save it back\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    stripped_lines = [line.strip() for line in lines]\n",
    "    \n",
    "    # Save the stripped lines to a new file (or overwrite the original if needed)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(stripped_lines))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
