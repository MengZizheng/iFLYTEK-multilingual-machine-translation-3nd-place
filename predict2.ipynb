{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bd4136-fce0-437a-9b12-b890ebb577a5",
   "metadata": {},
   "source": [
    "# 在验证集上微调3个epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9bbf7e2-9514-450d-a81c-be34022efb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 4365.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "en-zh\n",
      "model is done!\n",
      "训练参数已设置完成！\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.417900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.330300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.314000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40, training_loss=0.402643746137619, metrics={'train_runtime': 22.788, 'train_samples_per_second': 109.707, 'train_steps_per_second': 1.755, 'total_flos': 133115412480000.0, 'train_loss': 0.402643746137619, 'epoch': 5.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import sacrebleu\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../user_data/bart_tokenizer\")\n",
    "\n",
    "# 读取source和target文件\n",
    "def load_txt_data(source_path, target_path):\n",
    "    with open(source_path, \"r\", encoding=\"utf-8\") as src_file, open(target_path, \"r\", encoding=\"utf-8\") as tgt_file:\n",
    "        source_sentences = [f\"<zh> {line.strip()} </s>\" for line in src_file.readlines()]\n",
    "        target_sentences = [line.strip() for line in tgt_file.readlines()]\n",
    "    return source_sentences, target_sentences\n",
    "\n",
    "data_files = {\n",
    "    \"source\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/val/中文/en-zh.txt\",\n",
    "    \"target\": \"../xfdata/多语言机器翻译挑战赛数据集更新（以此测试集提交得分为准）/val/其他语言/en-zh.txt\"\n",
    "}\n",
    "\n",
    "# 加载txt文件中的句子\n",
    "source_sentences, target_sentences = load_txt_data(data_files[\"source\"], data_files[\"target\"])\n",
    "\n",
    "# 将数据转换为datasets格式\n",
    "dataset_dict = {\"source\": source_sentences, \"target\": target_sentences}\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "# Tokenize 函数\n",
    "def tokenize_function(examples):\n",
    "    source_texts = examples[\"source\"]\n",
    "    target_texts = examples[\"target\"]\n",
    "\n",
    "    # Tokenize source texts\n",
    "    model_inputs = tokenizer(source_texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    \n",
    "    # Tokenize target texts without using as_target_tokenizer context\n",
    "    labels = tokenizer(target_texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    # 将 labels 直接添加到 model_inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # 转换成字典格式，便于 datasets 库使用\n",
    "    return {key: value.tolist() for key, value in model_inputs.items()}\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(\"Data loaded.\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"en-zh\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"../user_data/step1/en/results/checkpoint-154690\").eval().to(device)\n",
    "print(\"model is done!\")\n",
    "\n",
    "# 6. 设置训练参数\n",
    "output_dir = \"../user_data/step1/en/continue\"\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"results\"),         # 训练结果保存路径\n",
    "    save_strategy=\"epoch\",                                   # 按步数进行保存\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_dir=os.path.join(output_dir, \"logs\"),           # 日志保存路径\n",
    "    learning_rate=5e-5,                                     # 学习率\n",
    "    per_device_train_batch_size=64,                         # 每个设备的训练批次大小\n",
    "    per_device_eval_batch_size=256,                         # 每个设备的验证批次大小\n",
    "    weight_decay=0.01,                                      # 权重衰减\n",
    "    save_total_limit=3,                                    # 保存的 checkpoint 数量上限\n",
    "    num_train_epochs=5,                                     # 训练 epoch 数\n",
    "    predict_with_generate=True,                             # 使用生成模式进行评估\n",
    "    bf16=True,                                              # 使用 bf16 精度\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"训练参数已设置完成！\")\n",
    "\n",
    "\n",
    "# 7. 使用 Seq2SeqTrainer 进行 微调\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                                     # 模型\n",
    "    args=training_args,                              # 训练参数\n",
    "    train_dataset=tokenized_dataset,           # 训练数据集\n",
    "    tokenizer=tokenizer,                             # 分词器\n",
    ")\n",
    "\n",
    "# 8. 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406794c6-5dce-4413-bfed-20f9f436716f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
